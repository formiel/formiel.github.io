[{"authors":["hang"],"categories":null,"content":"Welcome to my blog! My name is Hang. I am from Vietnam. I am currently a MSc student in Data Sciences and Business Analytics at CentraleSupélec \u0026amp; ESSEC Business School.\n","date":1558915200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1558915200,"objectID":"cc783102a175d3100eb446d2cfd78234","permalink":"https://hangle.fr/authors/hang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hang/","section":"authors","summary":"Welcome to my blog! My name is Hang. I am from Vietnam. I am currently a MSc student in Data Sciences and Business Analytics at CentraleSupélec \u0026amp; ESSEC Business School.","tags":null,"title":"Hang Le","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://hangle.fr/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Hang Le","type":"authors"},{"authors":["Hang Le"],"categories":["NLP","learning"],"content":"Stanford CS224n: Natural Language Processing with Deep Learning has been an excellent course in NLP for the last few years. Recently its 2019 edition lecture videos have been made publicly available. Therefore, I decided to \u0026ldquo;attend\u0026rdquo; this course. My objective is to follow closely the proposed schedule: two lectures and one assignment per week. My schedule will then be as follows.\n Assignment 1: Introduction to word vectors. Due May 28th. Assignment 2: Derivatives and implementation of word2vec algorithm. Due June 4th. Assignment 3: Dependency parsing and neural network foundations. Due June 11th. Assignment 4: Neural Machine Translation with sequence-to-sequence and attention. Due June 18th. Assignment 5: Neural Machine Translation with ConvNets and subword modeling. Due June 25th.  The code will be updated regularly in this repo.\n","date":1558915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558915200,"objectID":"8c765badc24eee3ddf3acc63b77ea0a1","permalink":"https://hangle.fr/post/cs224n/","publishdate":"2019-05-27T00:00:00Z","relpermalink":"/post/cs224n/","section":"post","summary":"I am going to update in this post my solutions to the assignments of Stanford CS224n.","tags":[],"title":"My Solutions to the Assignments of Stanford CS224n NLP Class","type":"post"},{"authors":["Hang Le"],"categories":["NLP"],"content":"Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB -- 1. Introduction - How to represent words Any machine learning model needs to be fed by data. Models learn and extract knowledge from data. Data could exist in various forms such as images, videos or text. However, no matter what type of data, they all have to be transformed into numerical values so that computers could understand and process them. For example, a grayscale image is seen by computers as a matrix of pixels, where each pixel is represented by an integer ranging from 0 to 255 depending on its intensity from black (0) to white (255). Color images, on the other hand, compose three values or channels for each pixel, in which each channel measures the intensity in one dimension of the color space (including red, green and blue dimensions) and hence are perceived as 3D arrays of integers.\nSo, how to enable computer to apprehend text data? Computers only see numbers, therefore we have to transform text into numbers. We could then break the text into smaller units similar to what we do with pixels for images. A natural choice is using an integer to represent each word. Thus a dictionary mapping every word with its corresponding integers and vice versa is maintained and used to encode words. However, unlike the pixels in images signifying the intensity in three different color dimensions, an integer could hardly tell us anything about its represented word such as word meaning, word senses, difference or similarity with other words. If a number is not sufficient to express a word, how about using a vector comprising many numbers where each number could encode some aspect of the word meaning? In fact, this is the most prominent method used to represent words to feed meaning-related models nowadays.\nThe earliest work of using vectors to represent words as points in high-dimensional space is the work of Osgood et al. (1957) on affective meanings or connotations, the aspects of a word's meaning that are related to emotions, sentiments, opinions or evaluations. Osgood et al. defined three dimensions over which a word could be expressed. These dimensions are:\n valence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus      Valence Arousal Dominance     courageous 8.05 5.5 7.38   music 7.67 5.57 6.5   heartbreak 2.45 5.65 3.58   cub 6.71 3.95 4.24   life 6.68 5.59 5.89    (source: Chapter 6: Vector Semantics - Speech and Language Processing. Dan Jurafsky and James H. Martin.)\nHowever, three dimensions is unlikely to convey multiple aspects of a word's meaning. But how should we build such a computational model that could capture different aspects of word meaning and transfer them into different dimension as we want? First, we need to define what a word meaning is. Linguists in the 1950's came up with a hypothesis called distributional hypothesis which emphasizes the role of context in word meaning.\n Words that occur in similar context tend to have similar meanings.\n It suggests that we could define a word by the environment or distribution it occurs in language use. This hypothesis is the fundamental concept lying at the heart of almost all words representation learning models. In fact, vector models based on distributional hypothesis are now the standard way to encode the meaning of words. There are two most commonly used models: count-based methods and Word2vec. I will briefly describe these two methods in Part 2 and Part 3. The detailed implementation of Skip-gram with Negative Sampling (SGNS), one of the algorithms of Word2vec, is presented in Part 4.\n2. Count-based methods The models using count-based methods are generally based on a co-occurrence matrix i.e. a matrix that counts how often words appear together.\n(1) Word-Document matrix\n Each row $i$ represents a word in the dictionary and each column $j$ represents a document. Each cell $(i, j)$ counts the number of times the word in row $i$ appears in the document $j$. Dimension: $V \\times M$, where $V$ is the vocabulary size and $M$ is the number of documents. Row vectors are word vectors. The dimension of word vectors is $M$.  The context that we choose to define a word in this case is documents. We can see that the choice of documents strongly affects the quality of the word embeddings (or word vectors).\n(2) Window-based co-occurrence matrix\n Each row $i$ represents a word and each column $j$ also represents a word in the dictionary. Each cell $(i, j)$ countes the number of times the (target) word in row $i$ and the (context) word in column $j$ co-occur in a window of $m$ words around the target word. Dimension: $V \\times V$, where $V$ is the vocabulary size Row or column vectors are word vectors. The dimension of word vectors is $V$.  The context now is not documents, but a window including $2m$ words (or less depending on the word's position) around the target word.\n(3) Term Frequency - Inverse Document Frequency (Tf-idf) weighted vectors\nThe simple raw frequency in the word-document and word-word matrices has some limitations in measuring the association between words when it comes to frequently used words like the, a, an, it, they etc. The ubiquitous words do not carry discriminative information and therefore are deemed to be unimportant. Term frequency - inverse document frequency (tf-idf) is an algorithm used to adjust a word frequency based on the following two terms:\n term frequency (tf): the frequency of the word ($t$) in the document ($d$). We use $\\log_{10}$ of the frequency to downweigh the raw frequency of a word. For example, a word has $tf=2$ if it appears 10 times, $tf=3$ if it appears 100 times etc. $$\\begin{equation*} tf_{t, d} = \\begin{cases} 1 + \\log_{10} \u0026amp; \\text{count($t, d$) } \\text{if count($t, d$)} \u0026gt; 0\\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} \\end{equation*}$$\n inverse document frequency (idf): inverse of the document frequency ($df_t$), which is the number of documents a word $t$ occurs in. It gives higher weights to words that occur only in a few documents since these terms may carry useful information to distinguish the documents from other documents in the collection. $$\\begin{equation*} idf_t = \\log_{10} \\left(\\frac{N}{df_t} \\right) \\end{equation*}$$\n  The tf-idf weighting of a word $t$ in document $d$, denotes as $w_{t, d}$, is the product of the two terms $tf_{t,d}$ and $idf_t$. $$\\begin{equation*} w_{t, d} = tf_{t, d} \\times idf_t \\end{equation*}$$\n(4) Positive Pointwise Mutual Information (PPMI) weighted matrix\nPPMI is an alternative weighting function to tf-idf. This method is based on PMI, a measure of association used in information theory. The intuition is that the association between 2 words $w$ and $c$ could be measured by how often these 2 words co-occur compared with the probability that they appear together purely by chance assuming they were independent. It is common to use positive values of PMI as negative ones is not intuitively interpretable. $$\\begin{equation*} PPMI(w,c) = \\text{max} \\left (\\log_2 \\frac{P(w,c)}{P(w)P(c)}, 0 \\right) \\end{equation*}$$\nAssuming we have a co-occurrence matrix $F$ with target words in rows and contexts words in columns. $f_{ij}$ denotes the elements at row $i$ and column $j$. $f_{ij}$ counts the number of times word $w_i$ occurs in context $c_j$. We have the PPMI value of word $w_i$ with context $c_j$ as follows: $$\\begin{equation*} P(w,c) = p_{ij} = \\frac{f_{ij}}{\\sum_{i}\\sum_{j} f_{ij}} \\end{equation*}$$ $$\\begin{align*} P(w) = p_{i*} = \\frac{\\sum_{j}f_{ij}}{\\sum_{i}\\sum_{j} f_{ij}} ; \\quad P(c) = p_{*j} = \\frac{\\sum_{i}f_{ij}}{\\sum_{i}\\sum_{j} f_{ij}} \\end{align*}$$ $$\\begin{equation*} PPMI_{ij} = \\text{max} \\left (\\log_2 \\frac{p_{ij}}{p_{i*}p_{*j}}, 0 \\right) \\end{equation*}$$\n3. Word2vec Word vectors constructed using count-based methods are highly sparse. The dimension is the vocabulary size $V$ with many zeros elements as one word normally does not co-occur with many other words in the dictionary. Although there exists algorithms to efficiently handle sparse matrices, the high dimensionality of word vectors would add complexity to the models using them as inputs. One may think of performing dimentionality reduction using Singular Value Decomposition (SVD), however it would be very costly (quadratic cost) to perform SVD since the vocabulary size $V$ is large (normally $10^6$).\nInstead of directly computing all co-occurrence word counts, iteration-based methods learn to encode the probability of a word given its context in one iteration at a time. The idea is to train a model with parameters are the word vectors themselves on some loss function. Iteratively, we update the parameters to reduce the loss and eventually learn our word representation vectors. Word2vec proposed by Mikolov et al. (2013) is an efficient method based on that principle to learn word embeddings. There are 2 algorithms presented in the Word2vec paper: continuous bag-of-words (CBOW) and skip-gram.\n(1) Continuous bag-of-words (CBOW)\nCBOW tries to predict a center word from its surrounding context. Let $w_i$ denotes the word $i$ from vocabulary. The parameters of our model consist of two matrices $\\V \\in \\RR^{n \\times V}$ and $\\U \\in \\RR^{V \\times n}$. $v_i$ is the $i$-th column of $\\V$, the input vector of word $w_i$. $u_i$ is the $i$-th row of $\\U$, the output vector of word $w_i$.\nIn CBOW model, the context vector $\\hat{v}$ is represented by taking average of the word context vectors $\\begin{equation}\\hat{v} = \\frac{v_{c-m} + \\dots + v_{c-1} + v_{c+1} + \\dots v_{c+m}}{2m} \\end{equation}$. We want to maximize the probability of the center word given its context of window $m$, which is equivalent to minimize the negative logarithm of that probability i.e. $\\begin{equation}P(u_c|\\hat{v}) \\end{equation}$.\nThe probability of a word vector given another word vector is computed using the softmax of the dot product of the 2 vectors. The dot product gives us a score. The higher this score is, the closer the 2 vectors or 2 words are together. The softmax operator turns this score into probability.\nThe objective function for one word is as follow: $$\\begin{align*} \\text{minimize } J \u0026amp;= - \\log P(w_c|w_{c-m}, \\dots , w_{c-1}, w_{c+1}, \\dots w_{c+m}) \\\\\\\\ \u0026amp;= - \\log P(u_c|\\hat{v}) \\\\\\\\ \u0026amp;= - \\log \\frac{\\exp(u_c ^\\top \\hat{v})}{\\sum_{j=1}^{V} \\exp(u_j ^\\top \\hat{v})} \\\\\\\\ \u0026amp;= -u_c ^\\top \\hat{v} + \\log \\sum_{j=1}^{V} \\exp(u_j ^\\top \\hat{v}) \\\\\\\\ \\end{align*}$$\nWe compute the derivatives of the loss function $J$ with respect to the parameters and update the parameters $\\U, \\V$ iteratively to eventually obtain the parameters, which are the word vectors themselves.\n(2) Skip-gram\nOpposite to CBOW, skip-gram tries to predict context words from a center word. Using similar notations to CBOW and based on the same principle, the objective function using skip-gram for one word is then formulated as follow: $$\\begin{align*} \\text{minimize } J \u0026amp;= - \\log P(w_{c-m}, \\dots , w_{c-1}, w_{c+1}, \\dots w_{c+m}|w_c) \\\\\\\\ \u0026amp;= - \\log \\prod_{j=0, j \\neq m}^{2m} P(w_{c-m+j}|w_c) \\\\\\\\ \u0026amp;= - \\log \\prod_{j=0, j \\neq m}^{2m} P(u_{c-m+j}|v_c) \\\\\\\\ \u0026amp;= - \\log \\prod_{j=0, j \\neq m}^{2m} \\frac{\\exp (u_{c-m+j}^ \\top) v_c}{\\sum_{k=1}^{V} \\exp(u_k ^\\top v_c)} \\\\\\\\ \u0026amp;= - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^\\top v_c + 2m \\log \\sum_{k=1}^{V} \\exp(u_k ^\\top v_c) \\end{align*}$$\nSimilar to CBOW, we update the parameters using stochastic gradient descent to obtain the word vectors.\n4. Implementation of Skip-gram with Negative Sampling (SGNS) The authors of Word2vec proposed negative sampling, a method to speed up the training process and improve the model performance in the paper Distributed Representations of Words and Phrases and their Compositionality (Tomas Mikolov et al.). Now, let me walk through the implementation of skip-gram with negative sampling where I take the derivatives and build the algorithms on my own from scratch using Numpy. You could refer to my code in the following Github repository.\nA main idea in negative sampling is that instead of looping over the entire vocabulary to compute the loss function, we could just take several negative examples from a noise distribution whose probabilities match the word frequency. We will use sigmoid function instead of softmax and the loss function to optimize is also changed. I will describe in details in the later part.\nBelow is the steps on how to implement SGNS:\n Obtain positive examples from the neigboring context of a target word Obtain negative examples by randomly sampling words in the lexicon based on a unigram distribution Train the model by optimizing the loss function Use the regression weights as the embedding vectors  Let $V$ denote the total number of words in our vocabulary and $d$ denote the embedding dimension (which is a hyperparameter). The parameters of the model consist of two matrices: $\\W\\in\\RR^{d\\times V}$ and $\\C\\in\\RR^{V\\times d}$. For any word (index) $t$ let $\\w_t,\\c_t$ respectively denote the $t^\\text{th}$ column of $\\W$ and the $t^\\text{th}$ \\textit{row} of $\\C$. The loss function over the whole training set can be expressed as:\n$$\\begin{equation} L(\\thetab) = \\sum_{(t,p) \\in +} - \\log \\frac{1}{1 + \\exp(-\\w_t^\\top \\c_p)} + \\sum_{(t,n) \\in -} - \\log \\frac{1}{1 + \\exp(\\w_t^\\top \\c_n)}, \\end{equation}$$ which can be understood as the negative log-likelihood of the data. The signs $+$ and $-$ respectively denote the set of positive and negative training instances.\nThe parameters of the model is $\\thetab = (\\W, \\C)$ and the objective is to minimize $L$ with respect to $\\W$ and $\\C$. A common solution is to use stochastic gradient descent, where in each iteration I minimize a local loss function with respect to a positive pair $(t,p)$: $$\\begin{equation}\\label{eq:loss-tp} L_{(t,p)} = -\\log \\frac{1}{1 + \\exp(-\\w_t^\\top \\c_p)} + \\sum_{n \\in \\cN(t,p)} - \\log \\frac{1}{1 + \\exp(\\w_t^\\top \\c_n)}, \\end{equation}$$ where $\\cN(t,p)$ denote the set of negative samples with respect to $t$, generated for the positive sample $p$ (for one positive sample I generate $K$ negative samples). With simple calculus, I obtain the following partial derivatives: $$\\begin{align} \\frac{\\partial{L_{(t,p)}}}{\\partial {\\w_t}} \u0026amp;= - s_p\\c_p + \\sum_{n \\in \\cN(t,p)} s_n\\c_n, \\\\ \\frac{\\partial{L_{(t,p)}}}{\\partial{\\c_p}} \u0026amp;= - s_p\\w_t, \\\\ \\frac{\\partial{L_{(t,p)}}}{\\partial{\\c_n}} \u0026amp;= s_n\\w_t \\quad\\forall n \\in \\cN(t,p) \\end{align}$$ where I have denoted \\begin{equation} s_p = \\frac{1}{1 + \\exp(\\w_t^\\top \\c_p)},\\quad s_n = \\frac{1}{1 + \\exp(-\\w_t^\\top \\c_n)}. \\end{equation} Once we have computed the above derivatives, we can apply gradient descent updates as follows: $\\begin{align} \\w_t \u0026amp;\\gets \\w_t - \\alpha \\frac{\\partial{L_{(t,p)}}}{\\partial {\\w_t}},\\label{eq:descent-wt}\\\\ \\c_p \u0026amp;\\gets \\c_p - \\alpha \\frac{\\partial{L_{(t,p)}}}{\\partial{\\c_p}},\\label{eq:descent-cp}\\\\ \\c_n \u0026amp;\\gets \\c_n - \\alpha \\frac{\\partial{L_{(t,p)}}}{\\partial{\\c_n}},\\label{eq:descent-cn} \\end{align}$ where $\\alpha$ is the step-size (which is another hyperparameter).\nAn epoch of the training loop can be sketched as follows:\n For each word $t$:  For each positive sample $p$ with respect to $t$: Get the set $\\cN(t,p)$ of negative samples (w.r.t. $t$). Compute the partial derivatives $\\frac{\\partial{L_{(t,p)}}}{\\partial {\\w_t}}, \\frac{\\partial{L_{(t,p)}}}{\\partial{\\c_p}}$ and $\\frac{\\partial{L_{(t,p)}}}{\\partial{\\c_n}}$. Update $\\w_t, \\c_p$ and $\\c_n$ using \\eqref{eq:descent-wt}, \\eqref{eq:descent-cp} and \\eqref{eq:descent-cn}. Compute the loss value \\eqref{eq:loss-tp} and add to the accumulated loss.   Below is the Python code of the inner loop. In the code, I used the variable $s_{neg}$ to denote a vector of all $s_n$, i.e. $\\set{s_n}_{n \\in \\cN(t,p)}$. For further detail please refer to the Github code.\n# context vector of the postive sample and the negative ones cp = C[p, :] C_neg = C[negative_samples, :] # intermediate values that are helpful sp = sigmoid(-np.dot(cp, wt)) s_neg = sigmoid(np.dot(C_neg, wt)) # Compute partial derivatives dwt = -sp*cp + np.dot(s_neg, C_neg) dcp = -sp*wt dC_neg = np.outer(s_neg, wt) # Gradient descent updates wt -= stepsize*dwt cp -= stepsize*dcp C_neg -= stepsize*dC_neg loss = -np.log(sigmoid(np.dot(cp, wt))) \\ + np.sum(-np.log(sigmoid(-np.dot(C_neg, wt)))) loss_epoch += loss  Another possible implementation can consist of only one loop:\n For each word $t$:  Get the set of $\\cP(t)$ of positive samples (w.r.t. $t$). Get the set $\\cN(t)$ of negative samples (w.r.t. $t$). Compute partial derivatives, gradient descent updates, and loss computation (as before).   This is indeed my first attempt but then I realized that this is not memory efficient (although it can be faster).\nI obtained the word vectors from the parameters of the matrix $\\W$. We could also get the word embeddings by taking average of the two matrices $\\W$ and $\\C$ or by concatenating them.\nModel performance: My SGNS model has been trained on the Billion Word Corpus and achieved a correlation of 0.9 with the human-annotated word similarity SimLex-999, a standard resource for the evaluation of word representation learning models.\nReferences  Efficient estimation of word representations in vector space. Mikolov, Tomas, et al. Distributed Representations of Words and Phrases and their Compositionality. Mikolov, Tomas, et al. Speech and Language Processing. Dan Jurafsky and James H. Martin. Lecture notes CS224N: Deep Learning for NLP.. Francois Chaubard, Rohit Mundra, Richard Socher. word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. Yoav Goldberg and Omer Levy.  ","date":1558137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558137600,"objectID":"c5bd9dd358c404da49ad8f0dc5507040","permalink":"https://hangle.fr/post/word2vec/","publishdate":"2019-05-18T00:00:00Z","relpermalink":"/post/word2vec/","section":"post","summary":"An introduction to understand word vectors (otherwise known as word embeddings) and to implement SGNS from scratch using Numpy.","tags":[],"title":"Understanding Word Vectors and Implementing Skip-gram with Negative Sampling","type":"post"},{"authors":["Hang Le"],"categories":["learning"],"content":" When I started studying data science, I spent a lot of time to find resources that best fit my background and my needs. The ones listed below have helped me tremendously in my study. I hope you will find some of them helpful in your learning as well.\n1. Programming -- -- A fundamental skill that any data scientist must have is programming. The most popular programming languages are without a doubt Python and R. You should learn both but if you have to pick one, choose Python. It has been the fastest-growing major language, and also the most wanted language, for the several years in a row.\nI strongly recommend MIT 6.0001 Introduction to Computer Science and Programming in Python, which is the most visited course of MIT OpenCourseWare.\n2. Probability and Statistics -- The lecture notes of MIT 18.05 Introduction to Probability and Statistics is an excellent study resource for a quick review of probability and statistics with clear and intuitive explanations. This is perhaps the best introduction to Probability and Statistics that I have ever seen.\nAfter finishing the above lecture notes, for a better coverage, I recommend you to continue with the book All of Statistics: A Concise Course in Statistical Inference by Larry A. Wasserman. The book provides a broad and as the name of the book indicated, concise coverage of probability and statistics, including basic and modern concepts that are closely related to machine learning.\nStudents who analyze data, or who aspire to develop new methods for analyzing data, should be well grounded in basic probability and mathematical statistics. Using fancy tools like neural nets, boosting, and support vector machines without understanding basic statistics is like doing brain surgery before knowing how to use a band-aid.  But where can students learn basic probability and statistics quickly? Nowhere. At least, that was my conclusion when my computer science colleagues kept asking me: “Where can I send my students to get a good understanding of modern statistics quickly?” The typical mathematical statistics course spends too much time on tedious and uninspiring topics (counting methods, two dimensional integrals, etc.) at the expense of covering modern concepts (bootstrapping, curve estimation, graphical models, etc.). -- 3. Linear Algebra MIT 18.06 Linear Algebra by Professor Gilbert Strang   -- Introduction to Linear Algebra, Fifth Edition   -- After struggling with two books borrowed from my school\u0026rsquo;s library, and also with the Linear Algebra series from Khan Academy, I could only appreciate the beauty of matrices thanks to the MIT 18.06 Linear Algebra course of Prof. Gilbert Strang. This is for many people, myself included, the best introduction to Linear Algebra. In parallel with watching the lectures, I would recommend you to do exercises in his book Introduction to Linear Algebra, 5th edition.\nIntroduction to Linear Algebra, Fifth Edition   -- 4. Optimization Optimization is of central importance in Data Science. I have learned many things from the Stanford EE364 Convex Optimization course (lecture videos are available on YouTube) and its accompanying textbook Convex Optimization by Stephen Boyd and Lieven Vandenberghe. However, I have to admit that I have finished only a small part of these resources as they were not so easy to digest. One of my main objectives for the next few months is to finish them, because I feel that what I have learned from my graduate program is really not enough.\n5. Machine Learning The book An Introduction to Statistical Learning with Applications in R (also known as ISL) is a very good read to any beginner.\nBesides, following the Professor Andrew Ng\u0026rsquo;s Machine Learning course has proved to be highly profitable to my learning. I love Prof. Andrew Ng\u0026rsquo;s teaching style where complicated concepts become crystal clear through his explanations. I found the practical assignments building algorithms from scratch in the course extremely useful to grasp the associated algorithms.\nFor an upper level, I would suggest two books, Pattern Recognition and Machine Learning by Christopher M. Bishop and The Elements of Statistical Learning by Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie. These two books are considered to be the best textbooks of Machine Learning by many people. I personally found it easier to start with ISL, though.\nIf you do a quick Google search, you will see that many people recommend the Deep Learning book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. I don't. It may be an excellent book for researchers, but for me it is not a good textbook for beginners. There are three resources that I strongly recommend to get started with Deep Learning. 1. Andrew Ng's [Coursera Deep Learning course](https://www.coursera.org/specializations/deep-learning). 2. PyTorch documentation -- Conclusion There is a large number of online resources and what works for some may not be found as useful by others. This is only recommendations based on my personal experience. I hope that you will find the ones that are best suited to you.\n","date":1557619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557619200,"objectID":"cb60d46fc6440f314b3b89d0ba891eb7","permalink":"https://hangle.fr/post/ds-beginner/","publishdate":"2019-05-12T00:00:00Z","relpermalink":"/post/ds-beginner/","section":"post","summary":"I share my experience in selecting the best resources to learn Data Science for an absolute beginner.","tags":[],"title":"Best Data Science Resources for Absolute Beginners","type":"post"},{"authors":["Hang Le"],"categories":[],"content":"Since I started learning Data Science, I have always wanted to start a blog on the topic. Unfortunately due to the large amount of work for my graduate study, this was not possible, until now.\nThis blog has two main purposes. First, I want to gain a deeper understanding of what I have learned; I find that writing is one of the best ways to learn. Second, I would like to share not only my knowledge but also my experience to the people, like myself a few months ago, who would likely be struggling when entering the field due to the lack of suitable prior background.\nComments and feedback are very much appreciated.\n","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"3f289ad9deff50f67a8232b628d934fb","permalink":"https://hangle.fr/post/hello-world/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/post/hello-world/","section":"post","summary":"About this blog","tags":[],"title":"Hello World!","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://hangle.fr/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"About me","tags":null,"title":"About","type":"widget_page"}]