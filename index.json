[{"authors":["hang"],"categories":null,"content":"Welcome to my blog! My name is Hang. I am from Vietnam. I am currently a MSc student in Data Sciences and Business Analytics at CentraleSupélec \u0026amp; ESSEC Business School.\n","date":1558137600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1558137600,"objectID":"cc783102a175d3100eb446d2cfd78234","permalink":"https://hangle.fr/authors/hang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hang/","section":"authors","summary":"Welcome to my blog! My name is Hang. I am from Vietnam. I am currently a MSc student in Data Sciences and Business Analytics at CentraleSupélec \u0026amp; ESSEC Business School.","tags":null,"title":"Hang Le","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://hangle.fr/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Hang Le","type":"authors"},{"authors":["Hang Le"],"categories":[],"content":"Many modelling tasks require text data as inputs. However, we could not just pass words or sentences to models. Computers could not be able to understand the text and hence learn from them. Therefore, featurizing the text, i.e. converting characters into numerical values, to feed the models plays a crucial role in machine learning. So, how to represent words using numbers?\n$\\newcommand{\\cN}{\\mathcal{N}}$\n$\\cN$\n\\[a+b\\]\nLet $V$ denote the total number of words in our vocabulary and $d$ denote the embedding dimension (which is a hyperparameter). The parameters of our model consist of two matrices: $\\W\\in\\RR^{d\\times V}$ and $\\C\\in\\RR^{V\\times d}$. For any word (index) $t$ let $\\w_t,\\c_t$ respectively denote the $t^\\text{th}$ column of $\\W$ and the $t^\\text{th}$ \\textit{row} of $\\C$. Our loss function over the whole training set can be expressed as:\n\\begin{equation} a + b + c + a + b + c + a + b + c + a + b + c + a + b + c + a + b + c + a + b + c + a + b + c + \\end{equation}\n$\\begin{align} L(\\thetab) = \u0026amp;\\sum_{(t,p) \\in +} - \\log \\frac{1}{1 + \\exp(-\\w_t^\\top \\c_p)} \\\\\\\\ \u0026amp; + a \\sum_{(t,n)} \\end{align}$\n$$\\begin{equation} L(\\thetab) = \\sum_{(t,p) \\in +} - \\log \\frac{1}{1 + \\exp(-\\w_t^\\top \\c_p)} + \\sum_{(t,n) \\in -} - \\log \\frac{1}{1 + \\exp(\\w_t^\\top \\c_n)}, \\end{equation}$$ which can be understood as the negative log-likelihood of the data. The signs $+$ and $-$ respectively denote the set of positive and negative training instances.\nThe parameters of our model is $\\thetab = (\\W, \\C)$ and our objective is to minimize $L$ with respect to $\\W$ and $\\C$. A common solution is to use stochastic gradient descent, where in each iteration we minimize a \\textit{local} loss function with respect to a positive pair $(t,p)$: $$\\begin{equation}\\label{eq:loss-tp} L_{(t,p)} = -\\log \\frac{1}{1 + \\exp(-\\w_t^\\top \\c_p)} + \\sum_{n \\in \\cN(t,p)} - \\log \\frac{1}{1 + \\exp(\\w_t^\\top \\c_n)}, \\end{equation}$$ where $\\cN(t,p)$ denote the set of negative samples with respect to $t$, generated for the positive sample $p$ (for positive sample we generate $K$ negative samples). With simple calculus, we obtain the following partial derivatives: $\\begin{align} \\pdv{L_{(t,p)}}{\\w_t} \u0026amp;= - s_p\\c_p + \\sum_{n \\in \\cN(t,p)} s_n\\c_n, \\\\ \\pdv{L_{(t,p)}}{\\c_p} \u0026amp;= - s_p\\w_t, \\\\ \\pdv{L_{(t,p)}}{\\c_n} \u0026amp;= s_n\\w_t \\quad\\forall n \\in \\cN(t,p) \\end{align}$ where we have denoted \\begin{equation} s_p = \\frac{1}{1 + \\exp(\\w_t^\\top \\c_p)},\\quad s_n = \\frac{1}{1 + \\exp(-\\w_t^\\top \\c_n)}. \\end{equation} Once we have computed the above derivatives, we can apply gradient descent updates as follows: $\\begin{align} \\w_t \u0026amp;\\gets \\w_t - \\alpha\\pdv{L_{(t,p)}}{\\w_t},\\label{eq:descent-wt}\\\\ \\c_p \u0026amp;\\gets \\c_p - \\alpha\\pdv{L_{(t,p)}}{\\c_p},\\label{eq:descent-cp}\\\\ \\c_p \u0026amp;\\gets \\c_p - \\alpha\\pdv{L_{(t,p)}}{\\c_p},\\label{eq:descent-cn} \\end{align}$ where $\\alpha$ is the step-size (which is another hyperparameter).\nFrom \\eqref{eq:loss-tp} we have\nReferences  Efficient estimation of word representations in vector space. Mikolov, Tomas, et al. Distributed Representations of Words and Phrases and their Compositionality. Mikolov, Tomas, et al. Speech and Language Processing. Dan Jurafsky and James H. Martin. Lecture notes CS224D: Deep Learning for NLP.. Francois Chaubard, Rohit Mundra, Richard Socher. word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. Yoav Goldberg and Omer Levy.  ","date":1558137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558137600,"objectID":"c5bd9dd358c404da49ad8f0dc5507040","permalink":"https://hangle.fr/post/word2vec/","publishdate":"2019-05-18T00:00:00Z","relpermalink":"/post/word2vec/","section":"post","summary":"An introduction to understand and implement skip-gram with negative sampling, one of the algorithms of Word2vec.","tags":[],"title":"Understanding and Implementing Skip-gram with Negative Sampling","type":"post"},{"authors":["Hang Le"],"categories":[],"content":" When I started studying data science, I had spent quite some time to find resources best fit to my background and my needs. The ones listed below have helped me tremendously in my study. I hope you will find some of them helpful in your learning as well.\n1. Linear Algebra MIT 18.06 Linear Algebra by Professor Gilbert Strang   -- I checked out one or two books borrowed from the school library and the Linear Algebra series in Khan Academy. However, I could only appreciate the beauty of matrices thanks to MIT 18.06 Linear Algebra course of Professor Gilbert Strang. I would recommend you to watch the lectures and doing exercises in his book Introduction to Linear Albera, 5th edition if you have time.\n2. Probability and Statistics The lecture notes of MIT 18.05 Introduction to Probability and Statistics by Profs. Jeremy Orloff and Jonathan Bloom is an excellent study resource for a quick review of probability and statistics with clear and intuitive explanations.\nIn addition, I also followed the book All of Statistics: A Concise Course in Statistical Inference by Larry A. Wasserman. The book provides a broad and as the name of the book indicated, concise coverage of probability and statistics. It covers basic and modern concepts in probability and statistics that are closely related to machine learning.\nStudents who analyze data, or who aspire to develop new methods for analyzing data, should be well grounded in basic probability and mathematical statistics. Using fancy tools like neural nets, boosting, and support vector machines without understanding basic statistics is like doing brain surgery before knowing how to use a band-aid.  But where can students learn basic probability and statistics quickly? Nowhere. At least, that was my conclusion when my computer science colleagues kept asking me: “Where can I send my students to get a good understanding of modern statistics quickly?” The typical mathematical statistics course spends too much time on tedious and uninspiring topics (counting methods, two dimensional integrals, etc.) at the expense of covering modern concepts (bootstrapping, curve estimation, graphical models, etc.). -- 3. Optimization Optimization is of central importance to almost all machine learning algorithms. Having an adequate understanding of optimization frameworks is essential to any machine learning and data science practitioners. The book that I recommend is Convex Optimization by Stephen Boyd and Lieven Vandenberghe. You can also follow the online courses instructed by Prof. Boyd via Stanford Lagunita or Youtube.\n4. Machine Learning The book An Introduction to Statistical Learning with Applications in R by Daniela Witten, Gareth James, Robert Tibshirani, and Trevor Hastie is a very good read to any beginner.\nBesides, following the Machine Learning course of Professor Andrew Ng on Coursera has proved to be highly profitable to my learning. I love Prof. Andrew Ng\u0026rsquo;s teaching style where complicated concepts become crystal clear through his explanations. I found the practical assignments building algorithms from scratch in the course extremely useful to grasp the associated algorithms.\nFor an upper level, I would suggest two books, Pattern Recognition and Machine Learning by Christopher M. Bishop and The Elements of Statistical Learning by Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie. These two books are considered to be the best textbooks of Machine Learning by many people.\nIn conclusion, there is a large number of online resources and what works for some may not be found as useful by others. This is only recommendations based on my personal experience. I hope that you will find the ones that are best suited to you.\n","date":1557964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557964800,"objectID":"cb60d46fc6440f314b3b89d0ba891eb7","permalink":"https://hangle.fr/post/ds-beginner/","publishdate":"2019-05-16T00:00:00Z","relpermalink":"/post/ds-beginner/","section":"post","summary":"I share my experience in selecting the best resources to learn Data Science for an absolute beginner.","tags":[],"title":"A Guide to Data Science for Absolute Beginners","type":"post"},{"authors":["Hang Le"],"categories":["coding"],"content":"One of the\u0026hellip; Using the argparse package.\n","date":1557532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557532800,"objectID":"30c3de54c16057ba63db157523de2606","permalink":"https://hangle.fr/post/argparse/","publishdate":"2019-05-11T00:00:00Z","relpermalink":"/post/argparse/","section":"post","summary":"One of the\u0026hellip; Using the argparse package.","tags":["Python","coding","argparse"],"title":"Passing arguments to a Python program","type":"post"},{"authors":["Hang Le"],"categories":[],"content":"Hello and welcome to my blog! Writing a blog is what I have wanted to do for a long time. However, I can only start this until now when I have fewer assignments in school.\nThrough this blog, I want to share my study experience as well as to talk about the projects that I implemented and what I have learned from them.\n","date":1557446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557446400,"objectID":"3f289ad9deff50f67a8232b628d934fb","permalink":"https://hangle.fr/post/hello-world/","publishdate":"2019-05-10T00:00:00Z","relpermalink":"/post/hello-world/","section":"post","summary":"About this blog","tags":[],"title":"Hello World!","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://hangle.fr/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"About me","tags":null,"title":"About","type":"widget_page"}]